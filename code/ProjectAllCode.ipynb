{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb5d3c-bd6f-450c-8971-d07582b35769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "from pennylane.qnn import TorchLayer\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import copy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchattacks import PGD\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fcda28-ca02-46bb-8963-b0663433e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_qubits = 4  # Changed from 2 to 4 qubits (2^4=16 amplitudes needed)\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "# Loss function definition\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2d41d-a543-41f1-b881-e6049e1e29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "dataset_config = {\n",
    "    'MNIST': {\n",
    "        'loader': datasets.MNIST,\n",
    "        'transform': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            transforms.RandomAffine(degrees=5, translate=(0.1, 0.1))\n",
    "        ]),\n",
    "        'input_channels': 1,\n",
    "        'num_classes': 10\n",
    "    },\n",
    "    'CIFAR100': {\n",
    "        'loader': datasets.CIFAR100,\n",
    "        'transform': transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "        ]),\n",
    "        'input_channels': 3,\n",
    "        'num_classes': 100\n",
    "    },\n",
    "    'STL10': {\n",
    "        'loader': datasets.STL10,\n",
    "        'transform': transforms.Compose([\n",
    "            transforms.Resize((96, 96)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(96, padding=8),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4467, 0.4398, 0.4066), (0.2603, 0.2566, 0.2713))\n",
    "        ]),\n",
    "        'input_channels': 3,\n",
    "        'num_classes': 10\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0490aed-99c7-41e4-8104-da9f8bd2a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Class with all requested modifications\n",
    "class MLVisualizer:\n",
    "    def __init__(self):\n",
    "        self.metrics_history = {\n",
    "            'Hybrid': {'train_loss': [], 'val_loss': [], 'accuracy': [], \n",
    "                      'precision': [], 'recall': [], 'f1': [], 'robustness': [],\n",
    "                      'time': [], 'cpu': [], 'memory': []},\n",
    "            'Classical': {'train_loss': [], 'val_loss': [], 'accuracy': [], \n",
    "                         'precision': [], 'recall': [], 'f1': [], 'robustness': [],\n",
    "                         'time': [], 'cpu': [], 'memory': []},\n",
    "            'Hybrid_Test': {'val_loss': [], 'accuracy': [], \n",
    "                           'precision': [], 'recall': [], 'f1': []},\n",
    "            'Classical_Test': {'val_loss': [], 'accuracy': [], \n",
    "                              'precision': [], 'recall': [], 'f1': []}\n",
    "        }\n",
    "    \n",
    "    def update_metrics(self, model_type, metrics_dict):\n",
    "        if model_type not in self.metrics_history:\n",
    "            print(f\"Warning: Model type '{model_type}' not found in metrics history\")\n",
    "            return\n",
    "            \n",
    "        for key, value in metrics_dict.items():\n",
    "            if key in self.metrics_history[model_type]:\n",
    "                self.metrics_history[model_type][key].append(value)\n",
    "            else:\n",
    "                print(f\"Warning: Metric '{key}' not found for model type '{model_type}'\")\n",
    "    \n",
    "    def plot_training_curves(self, base_filename='training_curves'):\n",
    "        \"\"\"Generate separate plots for each metric type\"\"\"\n",
    "        # Plot loss curves - now separate plots\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.metrics_history['Hybrid']['train_loss'], 'b-', label='Hybrid Train')\n",
    "        plt.plot(self.metrics_history['Hybrid']['val_loss'], 'b--', label='Hybrid Val')\n",
    "        plt.plot(self.metrics_history['Classical']['train_loss'], 'r-', label='Classical Train')\n",
    "        plt.plot(self.metrics_history['Classical']['val_loss'], 'r--', label='Classical Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training/Validation Loss Comparison')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{base_filename}_loss.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot accuracy - separate plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.metrics_history['Hybrid']['accuracy'], 'g-', label='Hybrid')\n",
    "        plt.plot(self.metrics_history['Classical']['accuracy'], 'm-', label='Classical')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Validation Accuracy Comparison')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{base_filename}_accuracy.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot F1 score - separate plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.metrics_history['Hybrid']['f1'], 'c-', label='Hybrid')\n",
    "        plt.plot(self.metrics_history['Classical']['f1'], 'y-', label='Classical')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score Comparison')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{base_filename}_f1.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot robustness if available - separate plot\n",
    "        if any(self.metrics_history['Hybrid']['robustness']):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            xvals = range(5, len(self.metrics_history['Hybrid']['robustness'])*5 +1, 5)\n",
    "            plt.plot(xvals, self.metrics_history['Hybrid']['robustness'], 'k-', label='Hybrid')\n",
    "            plt.plot(xvals, self.metrics_history['Classical']['robustness'], 'k--', label='Classical')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Robustness (%)')\n",
    "            plt.title('Adversarial Robustness Comparison')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{base_filename}_robustness.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    def plot_resource_usage(self):\n",
    "        \"\"\"Separate plots for each resource metric\"\"\"\n",
    "        # Time comparison\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.metrics_history['Hybrid']['time'], 'b-', label='Hybrid')\n",
    "        plt.plot(self.metrics_history['Classical']['time'], 'r-', label='Classical')\n",
    "        plt.title('Training Time per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Time (s)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('resource_time.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # CPU usage\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.metrics_history['Hybrid']['cpu'], 'b-', label='Hybrid')\n",
    "        plt.plot(self.metrics_history['Classical']['cpu'], 'r-', label='Classical')\n",
    "        plt.title('CPU Usage per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('CPU Usage (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('resource_cpu.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Memory usage\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.metrics_history['Hybrid']['memory'], 'b-', label='Hybrid')\n",
    "        plt.plot(self.metrics_history['Classical']['memory'], 'r-', label='Classical')\n",
    "        plt.title('Memory Usage per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Memory (GB)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('resource_memory.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_confusion_matrix(self, model, data_loader, device, class_names, model_name):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in data_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig(f'confusion_matrix_{model_name}.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_class_distribution(self, dataset, title):\n",
    "        if hasattr(dataset, 'targets'):\n",
    "            targets = dataset.targets\n",
    "        else:\n",
    "            targets = [y for _, y in dataset]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.countplot(x=targets)\n",
    "        plt.title(f'Class Distribution - {title}')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig('class_distribution.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_feature_space(self, model, data_loader, device, model_name, n_samples=1000):\n",
    "        model.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(data_loader):\n",
    "                if i * batch_size >= n_samples:\n",
    "                    break\n",
    "                data = data.to(device)\n",
    "                \n",
    "                if isinstance(model, EnhancedHybridModel):\n",
    "                    # Get quantum-enhanced features\n",
    "                    x = model.classical_net(data)\n",
    "                    x = x.view(x.size(0), -1)\n",
    "                    x = model.feature_reducer(x)\n",
    "                    x = model.qlayer(x)  # Quantum transformed features\n",
    "                else:\n",
    "                    # Get deep classical features (before final layer)\n",
    "                    x = model.conv_net(data)\n",
    "                    x = x.view(x.size(0), -1)\n",
    "                    if hasattr(model, 'head'):\n",
    "                        for layer in list(model.head.children())[:-1]:\n",
    "                            x = layer(x)\n",
    "                \n",
    "                features.append(x.cpu().numpy())\n",
    "                labels.append(target.cpu().numpy())\n",
    "        \n",
    "        features = np.concatenate(features)[:n_samples]\n",
    "        labels = np.concatenate(labels)[:n_samples]\n",
    "        \n",
    "        # Separate plots for PCA and t-SNE\n",
    "        self._plot_pca(features, labels, model_name)\n",
    "        self._plot_tsne(features, labels, model_name)\n",
    "        self._plot_decision_boundaries(features, labels, model_name)\n",
    "    \n",
    "    def _plot_pca(self, features, labels, model_name):\n",
    "        \"\"\"Separate PCA plot with decision boundaries\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pca = PCA(n_components=2)\n",
    "        features_pca = pca.fit_transform(features)\n",
    "        \n",
    "        # Plot decision boundaries\n",
    "        self._plot_decision_surface(features_pca, labels)\n",
    "        \n",
    "        scatter_pca = plt.scatter(features_pca[:, 0], features_pca[:, 1], c=labels, \n",
    "                                cmap='tab10', alpha=0.6, edgecolors='w', s=40)\n",
    "        plt.title(f'{model_name} Feature Space - PCA\\n'\n",
    "                 f'Explained Variance: {pca.explained_variance_ratio_.sum():.2f}')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.colorbar(scatter_pca, label='Class')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_space_pca_{model_name}.eps', format='eps', \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_tsne(self, features, labels, model_name):\n",
    "        \"\"\"Separate t-SNE plot with decision boundaries\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "        features_tsne = tsne.fit_transform(features)\n",
    "        \n",
    "        # Plot decision boundaries\n",
    "        self._plot_decision_surface(features_tsne, labels)\n",
    "        \n",
    "        scatter_tsne = plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=labels,\n",
    "                                 cmap='tab10', alpha=0.6, edgecolors='w', s=40)\n",
    "        plt.title(f'{model_name} Feature Space - t-SNE')\n",
    "        plt.xlabel('t-SNE Dimension 1')\n",
    "        plt.ylabel('t-SNE Dimension 2')\n",
    "        plt.colorbar(scatter_tsne, label='Class')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_space_tsne_{model_name}.eps', format='eps', \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_decision_surface(self, X, y):\n",
    "        \"\"\"Helper to plot decision boundaries\"\"\"\n",
    "        h = 0.02  # step size in the mesh\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        # Fit SVM to plot decision boundaries\n",
    "        svm = SVC(kernel='rbf', gamma=2)\n",
    "        svm.fit(X, y)\n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot decision boundaries\n",
    "        plt.contourf(xx, yy, Z, alpha=0.1, cmap='tab10')\n",
    "        plt.contour(xx, yy, Z, colors='k', linewidths=0.5, alpha=0.5)\n",
    "    \n",
    "    def _plot_decision_boundaries(self, features, labels, model_name):\n",
    "        \"\"\"Separate plot showing class separation boundaries\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pca = PCA(n_components=2)\n",
    "        features_pca = pca.fit_transform(features)\n",
    "        \n",
    "        # Fit SVM and plot decision boundaries\n",
    "        self._plot_decision_surface(features_pca, labels)\n",
    "        \n",
    "        # Plot data points\n",
    "        scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], c=labels,\n",
    "                            cmap='tab10', alpha=0.8, edgecolors='w', s=40)\n",
    "        \n",
    "        # Calculate separation metrics\n",
    "        lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "        lda.fit(features, labels)\n",
    "        separation_score = lda.score(features, labels)\n",
    "        sil_score = silhouette_score(features, labels)\n",
    "        \n",
    "        plt.title(f'{model_name} Class Separation\\n'\n",
    "                 f'LDA Separation: {separation_score:.3f} | Silhouette Score: {sil_score:.3f}')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.colorbar(scatter, label='Class')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'class_separation_{model_name}.eps', format='eps', \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_quantum_circuit(self, quantum_circuit, weights):\n",
    "        dummy_input = torch.zeros(2**num_qubits)  # For amplitude encoding\n",
    "        dummy_weights = torch.randn((num_layers, num_qubits))\n",
    "        fig, ax = qml.draw_mpl(quantum_circuit)(dummy_input, dummy_weights)\n",
    "        plt.title('Quantum Circuit Architecture')\n",
    "        plt.savefig('quantum_circuit.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_metric_comparison(self):\n",
    "        metrics = ['accuracy', 'f1', 'robustness', 'time']\n",
    "        hybrid_metrics = []\n",
    "        classical_metrics = []\n",
    "        \n",
    "        for m in metrics:\n",
    "            if m in self.metrics_history['Hybrid'] and self.metrics_history['Hybrid'][m]:\n",
    "                hybrid_metrics.append(np.mean(self.metrics_history['Hybrid'][m]))\n",
    "            if m in self.metrics_history['Classical'] and self.metrics_history['Classical'][m]:\n",
    "                classical_metrics.append(np.mean(self.metrics_history['Classical'][m]))\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(x - width/2, hybrid_metrics, width, label='Hybrid')\n",
    "        plt.bar(x + width/2, classical_metrics, width, label='Classical')\n",
    "        \n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(x, metrics)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.savefig('metric_comparison.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_test_results(self):\n",
    "        if not self.metrics_history['Hybrid_Test']['accuracy']:\n",
    "            print(\"No test results to plot\")\n",
    "            return\n",
    "            \n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        hybrid_metrics = [self.metrics_history['Hybrid_Test'][m][-1] for m in metrics]\n",
    "        classical_metrics = [self.metrics_history['Classical_Test'][m][-1] for m in metrics]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(x - width/2, hybrid_metrics, width, label='Hybrid')\n",
    "        plt.bar(x + width/2, classical_metrics, width, label='Classical')\n",
    "        \n",
    "        plt.title('Test Set Performance Comparison')\n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(x, metrics)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.savefig('test_results.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_sample_images(self, data_loader, title, n_images=10):\n",
    "        \"\"\"Plot sample images from dataset\"\"\"\n",
    "        data_iter = iter(data_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        \n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i in range(n_images):\n",
    "            plt.subplot(1, n_images, i+1)\n",
    "            if images[i].shape[0] == 1:  # Grayscale\n",
    "                plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "            else:  # RGB\n",
    "                plt.imshow(np.transpose(images[i].numpy(), (1, 2, 0)))\n",
    "            plt.title(f\"Label: {labels[i].item()}\")\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'sample_images_{title.lower().replace(\" \", \"_\")}.eps', \n",
    "                   format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "    def _plot_predictions(self, model, data_loader, device, model_name, n_images=10):\n",
    "        \"\"\"Plot sample predictions with true and predicted labels\"\"\"\n",
    "        model.eval()\n",
    "        data_iter = iter(data_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images[:n_images].to(device), labels[:n_images].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i in range(n_images):\n",
    "            plt.subplot(1, n_images, i+1)\n",
    "            if images[i].shape[0] == 1:  # Grayscale\n",
    "                plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "            else:  # RGB\n",
    "                plt.imshow(np.transpose(images[i].cpu().numpy(), (1, 2, 0)))\n",
    "            \n",
    "            title_color = 'green' if preds[i] == labels[i] else 'red'\n",
    "            plt.title(f\"T:{labels[i].item()}\\nP:{preds[i].item()}\", color=title_color)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Sample Predictions - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'predictions_{model_name}.eps', format='eps', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377ae63-a259-468c-91e4-14c72bf2353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(dataset_name, batch_size):\n",
    "    config = dataset_config[dataset_name]\n",
    "    \n",
    "    # Find the Normalize transform in the composition\n",
    "    normalize_transform = None\n",
    "    for t in config['transform'].transforms:\n",
    "        if isinstance(t, transforms.Normalize):\n",
    "            normalize_transform = t\n",
    "            break\n",
    "    \n",
    "    if not normalize_transform:\n",
    "        raise ValueError(\"Normalize transform not found in the transform composition\")\n",
    "    \n",
    "    # Full training set with augmentations\n",
    "    if dataset_name == 'STL10':\n",
    "        # STL10 has separate train and test splits\n",
    "        train_data = config['loader'](\n",
    "            './data', \n",
    "            split='train', \n",
    "            download=True,\n",
    "            transform=config['transform']\n",
    "        )\n",
    "        val_data = config['loader'](\n",
    "            './data',\n",
    "            split='test',\n",
    "            download=True,\n",
    "            transform=config['transform']\n",
    "        )\n",
    "        # For STL10, we'll use the provided test split as our validation set\n",
    "        # and create a smaller validation set from the training data\n",
    "        train_size = int(0.8 * len(train_data))\n",
    "        val_size = len(train_data) - train_size\n",
    "        train_data, _ = random_split(train_data, [train_size, val_size])\n",
    "    else:\n",
    "        # For MNIST and CIFAR100\n",
    "        train_data = config['loader'](\n",
    "            './data', \n",
    "            train=True, \n",
    "            download=True,\n",
    "            transform=config['transform']\n",
    "        )\n",
    "        # Split train into train and validation (80/20)\n",
    "        train_size = int(0.8 * len(train_data))\n",
    "        val_size = len(train_data) - train_size\n",
    "        train_data, val_data = random_split(\n",
    "            train_data, [train_size, val_size]\n",
    "        )\n",
    "    \n",
    "    # Test set - completely unseen data with no augmentations\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(normalize_transform.mean, \n",
    "                           normalize_transform.std)\n",
    "    ])\n",
    "    \n",
    "    if dataset_name == 'STL10':\n",
    "        test_data = config['loader'](\n",
    "            './data', \n",
    "            split='test', \n",
    "            transform=test_transform\n",
    "        )\n",
    "    else:\n",
    "        test_data = config['loader'](\n",
    "            './data', \n",
    "            train=False, \n",
    "            transform=test_transform\n",
    "        )\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7e039-190a-4f6d-90e9-a1de2686b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantum_device():\n",
    "    try:\n",
    "        dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "        print(\"Using default qubit simulator\")\n",
    "        return dev\n",
    "    except Exception as e:\n",
    "        print(f\"Could not initialize quantum device: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06bf14-526c-457b-a76c-31620739b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_circuit(inputs, weights):\n",
    "    \"\"\"Quantum circuit with amplitude encoding for 4 qubits (16 amplitudes)\"\"\"\n",
    "    # Normalize inputs for amplitude encoding (must be positive and normalized)\n",
    "    inputs = torch.abs(inputs)  # Ensure positive values\n",
    "    inputs = inputs / torch.norm(inputs)  # Normalize to unit vector\n",
    "    \n",
    "    # Amplitude embedding of classical data (now needs 16 values)\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(num_qubits), normalize=True, pad_with=0.)\n",
    "    \n",
    "    # Variational layers\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(num_qubits))\n",
    "    \n",
    "    # Measurement\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecfb1b-fdcf-4f62-890c-2a12b3eab1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedHybridModel(nn.Module):\n",
    "    def __init__(self, dataset_name):\n",
    "        super().__init__()\n",
    "        config = dataset_config[dataset_name]\n",
    "        self.input_channels = config['input_channels']\n",
    "        self.num_classes = config['num_classes']\n",
    "        \n",
    "        if dataset_name == 'MNIST':\n",
    "            self.classical_net = nn.Sequential(\n",
    "                nn.Conv2d(self.input_channels, 16, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.AdaptiveAvgPool2d((4, 4))\n",
    "            )\n",
    "            # Modified for 4-qubit amplitude encoding (output must be 2**4 = 16 values)\n",
    "            self.feature_reducer = nn.Sequential(\n",
    "                nn.Linear(32*4*4, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 16),  # 2^4 = 16 amplitudes needed\n",
    "                nn.Softmax(dim=1)    # Ensure valid probability amplitudes\n",
    "            )\n",
    "        elif dataset_name == 'CIFAR100':\n",
    "            self.classical_net = nn.Sequential(\n",
    "                nn.Conv2d(self.input_channels, 32, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4, 4))\n",
    "            )\n",
    "            # Modified for 4-qubit amplitude encoding\n",
    "            self.feature_reducer = nn.Sequential(\n",
    "                nn.Linear(128*4*4, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 16),  # 2^4 = 16 amplitudes needed\n",
    "                nn.Softmax(dim=1)    # Ensure valid probability amplitudes\n",
    "            )\n",
    "        else:  # STL10\n",
    "            self.classical_net = nn.Sequential(\n",
    "                nn.Conv2d(self.input_channels, 32, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4, 4))\n",
    "            )\n",
    "            # Modified for 4-qubit amplitude encoding\n",
    "            self.feature_reducer = nn.Sequential(\n",
    "                nn.Linear(256*4*4, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 16),  # 2^4 = 16 amplitudes needed\n",
    "                nn.Softmax(dim=1)    # Ensure valid probability amplitudes\n",
    "            )\n",
    "        \n",
    "        # Initialize quantum layer\n",
    "        try:\n",
    "            dev = get_quantum_device()\n",
    "            qnode = qml.QNode(quantum_circuit, dev, interface=\"torch\")\n",
    "            weight_shapes = {\"weights\": (num_layers, num_qubits)}\n",
    "            self.qlayer = TorchLayer(qnode, weight_shapes)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Quantum layer initialization failed: {str(e)}\")\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_qubits, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classical_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.feature_reducer(x)  # Now outputs 16 amplitudes\n",
    "        q_out = self.qlayer(x)      # Amplitude-encoded quantum processing (4 qubits)\n",
    "        return self.head(q_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0384106-9f5c-4985-a037-7b3451032c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedClassicalCNN(nn.Module):\n",
    "    def __init__(self, dataset_name):\n",
    "        super().__init__()\n",
    "        config = dataset_config[dataset_name]\n",
    "        self.input_channels = config['input_channels']\n",
    "        self.num_classes = config['num_classes']\n",
    "        \n",
    "        if dataset_name == 'MNIST':\n",
    "            self.conv_net = nn.Sequential(\n",
    "                nn.Conv2d(self.input_channels, 32, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.AdaptiveAvgPool2d((4, 4))\n",
    "            )\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(64*4*4, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(128, self.num_classes)\n",
    "            )\n",
    "        elif dataset_name == 'CIFAR100':\n",
    "            self.conv_net = nn.Sequential(\n",
    "                nn.Conv2d(self.input_channels, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        else:  # STL10\n",
    "            self.conv_net = nn.Sequential(\n",
    "                nn.Conv2d(self.input_channels, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbddf2-baff-40b3-8565-f76133301456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceTracker:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.start_cpu = None\n",
    "        self.gpus = GPUtil.getGPUs() if torch.cuda.is_available() else []\n",
    "    \n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        self.start_cpu = psutil.cpu_percent(interval=None)\n",
    "    \n",
    "    def end(self):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        cpu_usage = psutil.cpu_percent(interval=None) - self.start_cpu\n",
    "        memory_usage = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
    "        \n",
    "        gpu_info = {}\n",
    "        if self.gpus:\n",
    "            for gpu in self.gpus:\n",
    "                gpu_info[f'GPU_{gpu.id}_load'] = gpu.load\n",
    "                gpu_info[f'GPU_{gpu.id}_mem'] = gpu.memoryUsed\n",
    "        \n",
    "        return {\n",
    "            'time_sec': elapsed,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_gb': memory_usage,\n",
    "            **gpu_info\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468611b-dc30-4660-8059-827c3d447a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log(model, device, train_loader, optimizer, scheduler, epoch, model_type, resource_tracker, dataset_name, visualizer=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    resource_tracker.start()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        if \"Hybrid\" in model_type:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    resources = resource_tracker.end()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{epochs}], {model_type} Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Training Resources - Time: {resources['time_sec']:.2f}s, CPU: {resources['cpu_usage']:.1f}%, Memory: {resources['memory_gb']:.2f}GB\")\n",
    "    \n",
    "    if visualizer:\n",
    "        visualizer.update_metrics(model_type, {\n",
    "            'train_loss': avg_train_loss,\n",
    "            'time': resources['time_sec'],\n",
    "            'cpu': resources['cpu_usage'],\n",
    "            'memory': resources['memory_gb']\n",
    "        })\n",
    "    \n",
    "    return avg_train_loss, resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fd700-d0d0-4bad-98c3-111ab9c1dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_log(model, device, test_loader, epoch, model_type, best_accuracy, dataset_name, visualizer=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.squeeze().cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    if correct > 0:\n",
    "        avg_method = \"macro\" if epoch > 5 else \"micro\"\n",
    "        precision = precision_score(all_targets, all_preds, average=avg_method, zero_division=0)\n",
    "        recall = recall_score(all_targets, all_preds, average=avg_method, zero_division=0)\n",
    "        f1 = f1_score(all_targets, all_preds, average=avg_method, zero_division=0)\n",
    "    else:\n",
    "        precision = recall = f1 = 0.0\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{epochs}], {model_type} Val Loss: {test_loss:.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, \"\n",
    "          f\"Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        print(f\"✨ {model_type} validation improved to {accuracy:.2f}%\")\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), f\"best_{model_type.lower().replace(' ', '_')}_{dataset_name}.pth\")\n",
    "\n",
    "    if visualizer:\n",
    "        visualizer.update_metrics(model_type, {\n",
    "            'val_loss': test_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e51b48-6d3e-45fb-a2fa-b74e301f30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adversarial_robustness(model, data_loader, device, epsilon=0.1):\n",
    "    model.eval()\n",
    "    attack = PGD(model, eps=epsilon, alpha=0.01, steps=10)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, target in data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        adv_data = attack(data, target)\n",
    "        output = model(adv_data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    robustness = 100. * correct / total\n",
    "    print(f\"Adversarial Robustness (ε={epsilon}): {robustness:.2f}%\")\n",
    "    return robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6bed41-6f0f-4252-b5dd-a537d21e6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset_name='MNIST'):\n",
    "    # Initialize visualizer with enhanced plotting capabilities\n",
    "    visualizer = MLVisualizer()\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(dataset_name, batch_size)\n",
    "    \n",
    "    # 1. Plot sample images before training (new)\n",
    "    print(\"\\n===== VISUALIZING DATASET =====\")\n",
    "    visualizer.plot_sample_images(train_loader, 'Training Samples')\n",
    "    visualizer.plot_sample_images(val_loader, 'Validation Samples')\n",
    "    visualizer.plot_sample_images(test_loader, 'Test Samples')\n",
    "    \n",
    "    # 2. Plot class distributions before training\n",
    "    visualizer.plot_class_distribution(train_loader.dataset, 'Training Set')\n",
    "    visualizer.plot_class_distribution(val_loader.dataset, 'Validation Set')\n",
    "    \n",
    "    # Initialize models\n",
    "    hybrid_model = EnhancedHybridModel(dataset_name).to(device)\n",
    "    classical_model = EnhancedClassicalCNN(dataset_name).to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    hybrid_optim = optim.AdamW(\n",
    "        hybrid_model.parameters(),\n",
    "        lr=0.001, weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    classical_optim = optim.Adam(\n",
    "        classical_model.parameters(),\n",
    "        lr=0.003, weight_decay=0.005\n",
    "    )\n",
    "    \n",
    "    # Schedulers\n",
    "    hybrid_scheduler = OneCycleLR(\n",
    "        hybrid_optim,\n",
    "        max_lr=0.002,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=epochs,\n",
    "        pct_start=0.3\n",
    "    )\n",
    "    \n",
    "    classical_scheduler = CosineAnnealingWarmRestarts(\n",
    "        classical_optim,\n",
    "        T_0=5,\n",
    "        T_mult=1,\n",
    "        eta_min=1e-5\n",
    "    )\n",
    "    \n",
    "    # Trackers\n",
    "    hybrid_best = 0\n",
    "    classical_best = 0\n",
    "    resource_tracker = ResourceTracker()\n",
    "    \n",
    "    print(\"\\n===== STARTING TRAINING =====\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Hybrid Model params: {sum(p.numel() for p in hybrid_model.parameters())}\")\n",
    "    print(f\"Classical Model params: {sum(p.numel() for p in classical_model.parameters())}\\n\")\n",
    "\n",
    "    best_hybrid_model = None\n",
    "    best_classical_model = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "        \n",
    "        # Hybrid training\n",
    "        hybrid_train_loss, hybrid_resources = train_and_log(\n",
    "            hybrid_model, device, train_loader, hybrid_optim, hybrid_scheduler, \n",
    "            epoch, \"Hybrid\", resource_tracker, dataset_name, visualizer\n",
    "        )\n",
    "        hybrid_val_loss, hybrid_acc, hybrid_prec, hybrid_rec, hybrid_f1, hybrid_best = test_and_log(\n",
    "            hybrid_model, device, val_loader, epoch, \"Hybrid\", hybrid_best, dataset_name, visualizer\n",
    "        )\n",
    "\n",
    "        # Classical training\n",
    "        classical_train_loss, classical_resources = train_and_log(\n",
    "            classical_model, device, train_loader, classical_optim, classical_scheduler,\n",
    "            epoch, \"Classical\", resource_tracker, dataset_name, visualizer\n",
    "        )\n",
    "        classical_val_loss, classical_acc, classical_prec, classical_rec, classical_f1, classical_best = test_and_log(\n",
    "            classical_model, device, val_loader, epoch, \"Classical\", classical_best, dataset_name, visualizer\n",
    "        )\n",
    "\n",
    "        # Store best models\n",
    "        if hybrid_acc == hybrid_best:\n",
    "            best_hybrid_model = copy.deepcopy(hybrid_model)\n",
    "        if classical_acc == classical_best:\n",
    "            best_classical_model = copy.deepcopy(classical_model)\n",
    "\n",
    "        # Adversarial evaluation (every 5 epochs)\n",
    "        if epoch % 5 == 0:\n",
    "            hybrid_robustness = evaluate_adversarial_robustness(hybrid_model, val_loader, device)\n",
    "            classical_robustness = evaluate_adversarial_robustness(classical_model, val_loader, device)\n",
    "            visualizer.update_metrics('Hybrid', {'robustness': hybrid_robustness})\n",
    "            visualizer.update_metrics('Classical', {'robustness': classical_robustness})\n",
    "\n",
    "        # Plot intermediate results every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            visualizer.plot_training_curves()\n",
    "            visualizer.plot_resource_usage()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "        print(f\"  Hybrid - Loss: {hybrid_train_loss:.4f}/{hybrid_val_loss:.4f}, Acc: {hybrid_acc:.2f}%\")\n",
    "        print(f\"  Classical - Loss: {classical_train_loss:.4f}/{classical_val_loss:.4f}, Acc: {classical_acc:.2f}%\")\n",
    "        print(f\"  Resources - Hybrid: {hybrid_resources['time_sec']:.2f}s, Classical: {classical_resources['time_sec']:.2f}s\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if epoch > 10:\n",
    "            if dataset_name == 'MNIST' and hybrid_acc < 80 and classical_acc < 90:\n",
    "                print(\"Early stopping due to poor performance\")\n",
    "                break\n",
    "            elif dataset_name == 'CIFAR100' and hybrid_acc < 5 and classical_acc < 15:\n",
    "                print(\"Early stopping due to poor performance\")\n",
    "                break\n",
    "            elif dataset_name == 'STL10' and hybrid_acc < 30 and classical_acc < 40:\n",
    "                print(\"Early stopping due to poor performance\")\n",
    "                break\n",
    "        \n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Final visualizations\n",
    "    print(\"\\n===== GENERATING FINAL VISUALIZATIONS =====\")\n",
    "    visualizer.plot_training_curves()\n",
    "    visualizer.plot_resource_usage()\n",
    "    visualizer.plot_metric_comparison()\n",
    "    \n",
    "    # Get class names\n",
    "    if dataset_name == 'MNIST':\n",
    "        class_names = [str(i) for i in range(10)]\n",
    "    elif dataset_name == 'CIFAR100':\n",
    "        class_names = [str(i) for i in range(100)]\n",
    "    else:  # STL10\n",
    "        class_names = ['airplane', 'bird', 'car', 'cat', 'deer', \n",
    "                     'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "    \n",
    "    # Final evaluation on unseen test data\n",
    "    print(\"\\n===== FINAL TEST EVALUATION (UNSEEN DATA) =====\")\n",
    "    \n",
    "    if best_hybrid_model:\n",
    "        print(\"\\nEvaluating Best Hybrid Model on Test Set:\")\n",
    "        test_loss, test_acc, test_prec, test_rec, test_f1, _ = test_and_log(\n",
    "            best_hybrid_model, device, test_loader, \n",
    "            epoch, \"Hybrid_Test\", 0, dataset_name, visualizer\n",
    "        )\n",
    "        # Plot confusion matrix\n",
    "        visualizer.plot_confusion_matrix(best_hybrid_model, test_loader, device, class_names, 'Hybrid')\n",
    "        \n",
    "        # Plot feature space with decision boundaries\n",
    "        visualizer.plot_feature_space(best_hybrid_model, test_loader, device, 'Hybrid')\n",
    "        \n",
    "        # Plot sample predictions\n",
    "        visualizer._plot_predictions(best_hybrid_model, test_loader, device, 'Hybrid')\n",
    "    \n",
    "    if best_classical_model:\n",
    "        print(\"\\nEvaluating Best Classical Model on Test Set:\")\n",
    "        test_loss, test_acc, test_prec, test_rec, test_f1, _ = test_and_log(\n",
    "            best_classical_model, device, test_loader, \n",
    "            epoch, \"Classical_Test\", 0, dataset_name, visualizer\n",
    "        )\n",
    "        # Plot confusion matrix\n",
    "        visualizer.plot_confusion_matrix(best_classical_model, test_loader, device, class_names, 'Classical')\n",
    "        \n",
    "        # Plot feature space with decision boundaries\n",
    "        visualizer.plot_feature_space(best_classical_model, test_loader, device, 'Classical')\n",
    "        \n",
    "        # Plot sample predictions\n",
    "        visualizer._plot_predictions(best_classical_model, test_loader, device, 'Classical')\n",
    "\n",
    "    # Plot quantum circuit\n",
    "    dummy_weights = torch.randn((num_layers, num_qubits))\n",
    "    visualizer.plot_quantum_circuit(quantum_circuit, dummy_weights)\n",
    "    \n",
    "    # Plot final test results comparison\n",
    "    visualizer.plot_test_results()\n",
    "\n",
    "    print(\"\\n===== FINAL RESULTS =====\")\n",
    "    print(f\"Best Hybrid Validation Accuracy: {hybrid_best:.2f}%\")\n",
    "    print(f\"Best Classical Validation Accuracy: {classical_best:.2f}%\")\n",
    "\n",
    "    # Save all plots\n",
    "    print(\"\\n===== GENERATED VISUALIZATIONS =====\")\n",
    "    print(\"1. Sample images before training (sample_images_*.eps)\")\n",
    "    print(\"2. Class distributions (class_distribution.eps)\")\n",
    "    print(\"3. Training curves (training_curves_*.eps)\")\n",
    "    print(\"4. Resource usage (resource_*.eps)\")\n",
    "    print(\"5. Metric comparisons (metric_comparison.eps)\")\n",
    "    print(\"6. Test results (test_results.eps)\")\n",
    "    print(\"7. Confusion matrices (confusion_matrix_*.eps)\")\n",
    "    print(\"8. Feature spaces (feature_space_*.eps)\")\n",
    "    print(\"9. Class separation boundaries (class_separation_*.eps)\")\n",
    "    print(\"10. Sample predictions (predictions_*.eps)\")\n",
    "    print(\"11. Quantum circuit (quantum_circuit.eps)\")\n",
    "\n",
    "    # Save model architectures\n",
    "    with open('model_architectures.txt', 'w') as f:\n",
    "        f.write(\"HYBRID MODEL ARCHITECTURE:\\n\")\n",
    "        f.write(str(hybrid_model))\n",
    "        f.write(\"\\n\\nCLASSICAL MODEL ARCHITECTURE:\\n\")\n",
    "        f.write(str(classical_model))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING ON MNIST DATASET\")\n",
    "        print(\"=\"*60)\n",
    "        main(dataset_name='MNIST')\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        print(\"\\nFinal troubleshooting steps:\")\n",
    "        print(\"1. Verify all variable names are consistent\")\n",
    "        print(\"2. Check all required parameters are passed between functions\")\n",
    "        print(\"3. For a clean environment:\")\n",
    "        print(\"   conda create -n qml python=3.9\")\n",
    "        print(\"   conda activate qml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3f13d-b162-4455-a300-c16d024903e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
